{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc365d5",
   "metadata": {},
   "source": [
    "### HIGH LEVEL OUTLINE\n",
    "- Pull data from text8 (and from HN if wanted)\n",
    "- Tokenise this data\n",
    "- Create dataset using text8 data and a rolling context window\n",
    "- Connect to WandB\n",
    "- Use env variables\n",
    "- Train model on dataset\n",
    "- Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb99645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\", \".data\")\n",
    "TRAIN_PROCESSED_FILENAME = \"hn_posts_train_processed.parquet\"\n",
    "TEXT8_ZIP_URL = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "TEXT8_FILENAME = \"text8\"\n",
    "TEXT8_EXPECTED_LENGTH = 100_000_000  # text8 is expected to be 100 million chars\n",
    "\n",
    "# manually set env vars also used in data ingest (see .env.example / db-utils.py)\n",
    "TITLES_FILE = \"hn_posts_titles.parquet\"\n",
    "MINIMAL_FETCH_ONLY_TITLES = True\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_text8(\n",
    "    cache_dir: Union[str, Path] = DATA_DIR,\n",
    "    text8_filename: str = TEXT8_FILENAME,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Ensure the Matt Mahoney 'text8' corpus is present locally, downloading it\n",
    "    once and caching it for subsequent runs.\n",
    "    \"\"\"\n",
    "    cache_dir = Path(cache_dir).expanduser().resolve()\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = cache_dir / f\"{text8_filename}.zip\"\n",
    "    txt_path = cache_dir / text8_filename\n",
    "\n",
    "    # txt already present? then we are done\n",
    "    if txt_path.exists():\n",
    "        logger.info(f\"text8 file found - using cached data at {txt_path}\")\n",
    "        return txt_path\n",
    "\n",
    "    # ensure full text8 file is present, and otherwise download and extract as necessary\n",
    "    if not zip_path.exists():\n",
    "        logger.info(f\"Downloading text8 corpus to {zip_path}...\")\n",
    "        urllib.request.urlretrieve(TEXT8_ZIP_URL, zip_path)\n",
    "    logger.info(f\"Extracting {zip_path} to {txt_path}...\")\n",
    "    with ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extract(text8_filename, cache_dir)\n",
    "    return txt_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f0fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt8_path = get_text8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb877803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PUNCTUATION_MAP = {\n",
    "    \"<\": \"<LESS>\",\n",
    "    \">\": \"<GREATER>\",\n",
    "    \",\": \"<COMMA>\",\n",
    "    \".\": \"<PERIOD>\",\n",
    "    \"!\": \"<EXCLAMATION>\",\n",
    "    \"?\": \"<QUESTION>\",\n",
    "    \":\": \"<COLON>\",\n",
    "    \";\": \"<SEMICOLON>\",\n",
    "    \"-\": \"<DASH>\",\n",
    "    \"(\": \"<LPAREN>\",\n",
    "    \")\": \"<RPAREN>\",\n",
    "    \"[\": \"<LBRACKET>\",\n",
    "    \"]\": \"<RBRACKET>\",\n",
    "    \"{\": \"<LBRACE>\",\n",
    "    \"}\": \"<RBRACE>\",\n",
    "    '\"': \"<QUOTE>\",\n",
    "    \"'\": \"<APOSTROPHE>\",\n",
    "    \"/\": \"<SLASH>\",\n",
    "    \"\\\\\": \"<BACKSLASH>\",\n",
    "    \"&\": \"<AMPERSAND>\",\n",
    "    \"@\": \"<AT>\",\n",
    "    \"#\": \"<HASH>\",\n",
    "    \"$\": \"<DOLLAR>\",\n",
    "    \"%\": \"<PERCENT>\",\n",
    "    \"*\": \"<ASTERISK>\",\n",
    "    \"+\": \"<PLUS>\",\n",
    "    \"=\": \"<EQUALS>\",\n",
    "    \"|\": \"<PIPE>\",\n",
    "    \"~\": \"<TILDE>\",\n",
    "    \"`\": \"<BACKTICK>\",\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: can we improve? e.g. remove stop words, stem/lemmatise\n",
    "def tokenise(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenises a long string of text by lowercasing, replacing punctuation with predefined angle bracket words.\n",
    "\n",
    "    Args:\n",
    "        text (str): A single string.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each word to a unique index.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace all punctuation with angle bracket words\n",
    "    for punct, replacement in PUNCTUATION_MAP.items():\n",
    "        text = text.replace(punct, f\" {replacement} \")\n",
    "\n",
    "    # Split into words (handles multiple spaces)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "\n",
    "def build_vocab(\n",
    "    tokens: list[str],\n",
    "    min_freq: int = 5,\n",
    "    subsampling_threshold: float = 1e-5,\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of words that appear more than the frequency threshold.\n",
    "    \"\"\"\n",
    "    word_counts = Counter(tokens)\n",
    "    # Remove words with frequency below threshold\n",
    "    token_list = [UNK_TOKEN] + [\n",
    "        word for word, count in word_counts.items() if count >= min_freq\n",
    "    ]\n",
    "    num_discarded_freq = len(word_counts) - len(token_list)\n",
    "\n",
    "    # Frequency subsampling (remove frequent words with probability proportional to their frequency)\n",
    "    total_count = sum(Counter(tokens).values())\n",
    "    subsampled = []\n",
    "    freqs = Counter(tokens)\n",
    "    discarded_count_subsampling = 0  # Counter for discarded tokens\n",
    "    for word in token_list:\n",
    "        if word == UNK_TOKEN:\n",
    "            subsampled.append(word)\n",
    "            continue\n",
    "        freq = freqs[word] / total_count\n",
    "        prob_discard = 1 - (subsampling_threshold / freq) ** 0.5\n",
    "        if random.random() > prob_discard:\n",
    "            subsampled.append(word)\n",
    "        else:\n",
    "            discarded_count_subsampling += 1  # Increment counter if discarded\n",
    "    token_list = subsampled\n",
    "\n",
    "    vocab = {word: idx for idx, word in enumerate(token_list)}\n",
    "\n",
    "    # Report\n",
    "    logger.info(f\"Total tokens in: {len(tokens)}\")\n",
    "    logger.info(f\"Number discarded from frequency threshold: {num_discarded_freq} ({num_discarded_freq / len(word_counts) * 100:.2f}%)\")\n",
    "    # logger.info(f\"Number discarded from subsampling: {discarded_count_subsampling} ({discarded_count_subsampling / len(token_list) * 100:.2f}%)\")\n",
    "    logger.info(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_tokens_as_indices(tokens: list[str], vocab: dict) -> list[int]:\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to their corresponding indices using the provided vocab mapping.\n",
    "    This is to ensure we have fast, random-access, constant-sized, GPU-friendly data upfront.\n",
    "    \"\"\"\n",
    "    unk = vocab[UNK_TOKEN]\n",
    "    return [vocab.get(t, unk) for t in tokens]\n",
    "\n",
    "\n",
    "def get_words_from_indices(indeces: list[int], vocab: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converts a list of token indeces to a list of token values\n",
    "    \"\"\"\n",
    "    return [\n",
    "        list(vocab.keys())[list(vocab.values()).index(idx)]\n",
    "        for idx in indeces\n",
    "        if idx in vocab.values()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5188da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(corpus, context_size):\n",
    "    pairs = []\n",
    "    for i in range(context_size, len(corpus) - context_size):\n",
    "        center = corpus[i]\n",
    "        context = corpus[i - context_size:i] + corpus[i + 1:i + context_size + 1]\n",
    "        for ctx in context:\n",
    "            pairs.append((center, ctx))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def build_sgram_dataset(context_size: int = 5):\n",
    "    # read text8 file\n",
    "    # Read the text8 file\n",
    "    with open(txt8_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    text_tokens = tokenise(text)\n",
    "\n",
    "    # Build the vocabulary\n",
    "    vocab = build_vocab(text_tokens, min_freq=0, subsampling_threshold=1e-4)\n",
    "    \n",
    "    print(text_tokens[:10])  # Print first 10 tokens for verification\n",
    "\n",
    "    text_token_inds = get_tokens_as_indices(text_tokens, vocab)\n",
    "\n",
    "    print(text_token_inds[:10])  # Print first 10 token indices for verification\n",
    "    # print(len(text_token_inds))\n",
    "\n",
    "    # Generate skip-gram pairs\n",
    "    skipgram_pairs = generate_skipgram_pairs(text_token_inds, context_size)\n",
    "    print(skipgram_pairs[:10])  # Print first 10 pairs for verification\n",
    "    \n",
    "    # print(len(skipgram_pairs))  # Print total number of pairs\n",
    "    return skipgram_pairs, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d2c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "[1, 2, 3, 0, 0, 0, 4, 0, 0, 0]\n",
      "[(0, 1), (0, 2), (0, 3), (0, 0), (0, 0), (0, 4), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "ds_pairs, ds_vocab = build_sgram_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819778b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ethanedwards/Library/Python/3.10/lib/python/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch scikit-learn tqdm\n",
    "import torch\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "logger = logging.getLogger(__name__)\n",
    "def train_skipgram_model(\n",
    "    skipgram_pairs: list[tuple[int, int]],\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a skip-gram model on the provided skipgram pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is a placeholder for the actual training logic.\n",
    "    # You would typically use a library like PyTorch or TensorFlow to implement the model.\n",
    "    logger.info(\"Training skip-gram model on provided pairs...\")\n",
    "\n",
    "    # Example: Use a neural network to learn word embeddings based on skipgram pairs\n",
    "    # For now, we just log the number of pairs\n",
    "    logger.info(f\"Number of skipgram pairs: {len(skipgram_pairs)}\")\n",
    "    return\n",
    "\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, skipgram_pairs):\n",
    "        self.pairs = skipgram_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.in_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center, context):\n",
    "        center_emb = self.in_embeddings(center)\n",
    "        context_emb = self.out_embeddings(context)\n",
    "        score = torch.sum(center_emb * context_emb, dim=1)\n",
    "        return score\n",
    "\n",
    "# Split skipgram_pairs into train, val, test sets (80/10/10 split)\n",
    "def split_dataset(pairs, seed=42):\n",
    "    train_pairs, temp_pairs = train_test_split(pairs, test_size=0.2, random_state=seed)\n",
    "    val_pairs, test_pairs = train_test_split(temp_pairs, test_size=0.5, random_state=seed)\n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "train_pairs, val_pairs, test_pairs = split_dataset(ds_pairs)\n",
    "\n",
    "def train_skipgram_epochs(skipgram_pairs, vocab_size, embedding_dim=100, batch_size=64, lr=0.01, epochs=1):\n",
    "    dataset = SkipGramDataset(skipgram_pairs)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for center, context in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            # Positive samples\n",
    "            pos_labels = torch.ones(center.size(0))\n",
    "            pos_score = model(center, context)\n",
    "            pos_loss = loss_fn(pos_score, pos_labels)\n",
    "            # Negative sampling\n",
    "            neg_context = torch.randint(0, vocab_size, context.size())\n",
    "            neg_labels = torch.zeros(center.size(0))\n",
    "            neg_score = model(center, neg_context)\n",
    "            neg_loss = loss_fn(neg_score, neg_labels)\n",
    "            loss = pos_loss + neg_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs} - Average loss: {avg_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Example test with dummy data for several epochs\n",
    "model = train_skipgram_epochs(train_pairs, len(ds_vocab), epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6586d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c89a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
